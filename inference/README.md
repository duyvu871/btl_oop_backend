# OpenAI-compatible Embeddings Server (BKAI)

M·ªôt m√°y ch·ªß embedding vƒÉn b·∫£n ƒë·∫°t chu·∫©n OpenAI API, s·ª≠ d·ª•ng m√¥ h√¨nh bi-encoder ti·∫øng Vi·ªát t·ª´ BKAI Foundation Models. H·ªá th·ªëng ƒë∆∞·ª£c thi·∫øt k·∫ø v·ªõi ki·∫øn tr√∫c OOP chu·∫©n, d·ªÖ m·ªü r·ªông v√† b·∫£o tr√¨.

## üéØ T√≠nh nƒÉng

- **OpenAI-compatible API**: T∆∞∆°ng th√≠ch v·ªõi giao di·ªán OpenAI Embeddings API
- **X·ª≠ l√Ω vƒÉn b·∫£n d√†i**: T·ª± ƒë·ªông chia nh·ªè v√† gh√©p n·ªëi embeddings cho c√°c vƒÉn b·∫£n v∆∞·ª£t qu√° gi·ªõi h·∫°n
- **H·ªó tr·ª£ GPU/CPU**: T·ª± ƒë·ªông ph√°t hi·ªán v√† s·ª≠ d·ª•ng GPU n·∫øu c√≥ s·∫µn
- **API Documentation**: Giao di·ªán Scalar UI t∆∞∆°ng t√°c cho vi·ªác th·ª≠ nghi·ªám API
- **Authentication**: H·ªó tr·ª£ Bearer token authentication
- **OOP Architecture**: C·∫•u tr√∫c code s·∫°ch v·ªõi separation of concerns

## üìã Y√™u c·∫ßu

- Python >= 3.10
- CUDA 11.8+ (t√πy ch·ªçn, ƒë·ªÉ s·ª≠ d·ª•ng GPU)

## üöÄ C√†i ƒë·∫∑t

### 1. C√†i ƒë·∫∑t Dependencies

S·ª≠ d·ª•ng `uv` (khuy·∫øn ngh·ªã):

```bash
cd inference
uv sync
```

Ho·∫∑c s·ª≠ d·ª•ng `pip`:

```bash
cd inference
pip install -r requirements.txt
```

### 2. C√†i ƒë·∫∑t PyTorch (n·∫øu c·∫ßn)

Cho **GPU (CUDA)**:
```bash
uv pip install torch==2.4.* torchvision==0.19.* torchaudio==2.4.*
```

Cho **CPU only**:
```bash
uv pip install torch==2.4.* --index-url https://download.pytorch.org/whl/cpu
```

## üì¶ C·∫•u tr√∫c D·ª± √°n

```
inference/
‚îú‚îÄ‚îÄ main.py              # ·ª®ng d·ª•ng ch√≠nh (3 classes: Config, EmbeddingModel, EmbeddingAPI)
‚îú‚îÄ‚îÄ pyproject.toml       # C·∫•u h√¨nh d·ª± √°n v√† dependencies
‚îú‚îÄ‚îÄ README.md           # T√†i li·ªáu n√†y
‚îú‚îÄ‚îÄ Dockerfile          # Docker image cho production
‚îú‚îÄ‚îÄ Dockerfile.dev      # Docker image cho development
‚îî‚îÄ‚îÄ resources/          # T√†i nguy√™n (n·∫øu c√≥)
```

## üèóÔ∏è Ki·∫øn tr√∫c OOP

D·ª± √°n ƒë∆∞·ª£c c·∫•u tr√∫c theo 3 class ch√≠nh:

### 1. **Config** - Qu·∫£n l√Ω C·∫•u h√¨nh

```python
class Config:
    """Loads configuration from environment variables"""
    - model_id: Hugging Face model identifier
    - device: 'cuda' ho·∫∑c 'cpu'
    - batch_size: K√≠ch th∆∞·ªõc batch
    - api_key: Bearer token (t√πy ch·ªçn)
    - max_length: ƒê·ªô d√†i max c·ªßa sequences
```

### 2. **EmbeddingModel** - X·ª≠ l√Ω Model

```python
class EmbeddingModel:
    """Manages the embedding model and text encoding"""
    - chunk_by_tokens(): Chia vƒÉn b·∫£n d√†i th√†nh chunks
    - embed_long_text(): Encode vƒÉn b·∫£n d√†i + mean pooling
    - encode_texts(): Encode danh s√°ch vƒÉn b·∫£n
```

### 3. **EmbeddingAPI** - API Endpoints

```python
class EmbeddingAPI:
    """FastAPI application with OpenAI-compatible endpoints"""
    - GET /health: Health check
    - GET /v1/models: Danh s√°ch models
    - POST /v1/embeddings: T·∫°o embeddings
    - GET /scalar: Scalar UI documentation
```

## üîß C·∫•u h√¨nh

### Bi·∫øn M√¥i tr∆∞·ªùng

| Bi·∫øn | M·∫∑c ƒë·ªãnh | M√¥ t·∫£ |
|------|----------|-------|
| `MODEL_ID` | `bkai-foundation-models/vietnamese-bi-encoder` | Hugging Face model identifier |
| `DEVICE` | Auto-detect | `cuda` ho·∫∑c `cpu` |
| `BATCH_SIZE` | `64` | K√≠ch th∆∞·ªõc batch encoding |
| `NORMALIZE` | `1` | Chu·∫©n h√≥a embeddings (0 ho·∫∑c 1) |
| `API_KEY` | `` (empty) | Bearer token (b·ªè tr·ªëng = kh√¥ng auth) |
| `MAX_LENGTH` | `256` | ƒê·ªô d√†i max sequence |

### V√≠ d·ª• `.env`

```bash
MODEL_ID=bkai-foundation-models/vietnamese-bi-encoder
DEVICE=cuda
BATCH_SIZE=64
NORMALIZE=1
API_KEY=your-secret-key-here
MAX_LENGTH=256
```

## üöÄ Kh·ªüi ch·∫°y

### 1. Development

```bash
python main.py
```

Server s·∫Ω ch·∫°y t·∫°i `http://localhost:8000`

Ho·∫∑c s·ª≠ d·ª•ng uvicorn tr·ª±c ti·∫øp v·ªõi auto-reload:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### 2. Production

**V·ªõi Gunicorn + Uvicorn workers**:

```bash
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000
```

Tham s·ªë:
- `-w 4`: 4 worker processes (t√πy ch·ªânh theo CPU cores)
- `-k uvicorn.workers.UvicornWorker`: S·ª≠ d·ª•ng uvicorn worker class
- `-b 0.0.0.0:8000`: Bind t·ªõi port 8000

**T·ªëi ∆∞u h√≥a cho GPU**:

```bash
# S·ªë workers = s·ªë GPU
gunicorn main:app -w 1 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000 \
  --timeout 120 \
  --access-logfile - \
  --error-logfile -
```

### 3. V·ªõi Docker

```bash
# Development
docker-compose -f docker-compose.dev.yml up

# Production
docker-compose -f docker-compose.prod.yml up
```

## ‚öôÔ∏è Gunicorn Configuration

### Worker Types

- **sync**: Default, ph√π h·ª£p cho I/O-bound (HTTP requests)
- **uvicorn.workers.UvicornWorker**: **Khuy·∫øn ngh·ªã** cho FastAPI (async/await support)
- **gevent**: Async framework (y√™u c·∫ßu `gevent` package)
- **tornado**: Async framework (y√™u c·∫ßu `tornado` package)

### T√≠nh to√°n s·ªë Workers

```python
# Formula: (2 √ó CPU cores) + 1
# V√≠ d·ª•: 4 CPU cores ‚Üí (2 √ó 4) + 1 = 9 workers

# Cho GPU inference:
# Workers = s·ªë GPU (v√¨ m·ªói worker chi·∫øm 1 GPU)
```

### C·∫•u h√¨nh Advanced

```bash
gunicorn main:app \
  -w 4 \
  -k uvicorn.workers.UvicornWorker \
  -b 0.0.0.0:8000 \
  --timeout 120 \                    # Timeout 120s (cho model loading)
  --access-logfile - \               # Log to stdout
  --error-logfile - \                # Error log to stdout
  --log-level debug \                # Log level
  --max-requests 1000 \              # Restart worker sau 1000 requests
  --max-requests-jitter 100          # Random jitter ƒë·ªÉ tr√°nh restart c√πng l√∫c
```

## üìö API Endpoints

### 1. Health Check

```bash
curl http://localhost:8000/health
```

**Response:**
```json
{
  "ok": true,
  "device": "cuda",
  "model_id": "bkai-foundation-models/vietnamese-bi-encoder"
}
```

### 2. Danh s√°ch Models

```bash
curl http://localhost:8000/v1/models \
  -H "Authorization: Bearer your-api-key"
```

**Response:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "bkai-foundation-models/vietnamese-bi-encoder",
      "object": "model",
      "created": 1729123456,
      "owned_by": "local"
    }
  ]
}
```

### 3. T·∫°o Embeddings

```bash
curl http://localhost:8000/v1/embeddings \
  -X POST \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "model": "bkai-foundation-models/vietnamese-bi-encoder",
    "input": ["Xin ch√†o", "B·∫°n kh·ªèe kh√¥ng"]
  }'
```

**Response:**
```json
{
  "object": "list",
  "data": [
    {
      "object": "embedding",
      "index": 0,
      "embedding": [-0.0234, 0.1245, ...]
    },
    {
      "object": "embedding",
      "index": 1,
      "embedding": [0.0123, -0.0456, ...]
    }
  ],
  "model": "bkai-foundation-models/vietnamese-bi-encoder",
  "usage": {
    "prompt_tokens": 0,
    "total_tokens": 0
  }
}
```

### 4. Scalar UI Documentation

Truy c·∫≠p giao di·ªán t∆∞∆°ng t√°c: `http://localhost:8000/scalar`

Giao di·ªán n√†y cung c·∫•p:
- Danh s√°ch t·∫•t c·∫£ endpoints
- Th·ª≠ nghi·ªám tr·ª±c ti·∫øp API
- Xem request/response examples
- T√†i li·ªáu chi ti·∫øt

## üí° V√≠ d·ª• S·ª≠ d·ª•ng

### Python

```python
import requests

API_URL = "http://localhost:8000/v1/embeddings"
API_KEY = "your-api-key"

texts = [
    "Xin ch√†o, ƒë√¢y l√† m·ªôt c√¢u ti·∫øng Vi·ªát",
    "T√¥i y√™u l·∫≠p tr√¨nh",
]

response = requests.post(
    API_URL,
    json={"input": texts},
    headers={"Authorization": f"Bearer {API_KEY}"}
)

embeddings = response.json()
for i, emb_data in enumerate(embeddings["data"]):
    print(f"Text {i}: {len(emb_data['embedding'])} dimensions")
```

### JavaScript

```javascript
const API_URL = "http://localhost:8000/v1/embeddings";
const API_KEY = "your-api-key";

const texts = [
    "Xin ch√†o, ƒë√¢y l√† m·ªôt c√¢u ti·∫øng Vi·ªát",
    "T√¥i y√™u l·∫≠p tr√¨nh",
];

const response = await fetch(API_URL, {
    method: "POST",
    headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${API_KEY}`,
    },
    body: JSON.stringify({ input: texts }),
});

const embeddings = await response.json();
console.log(embeddings.data);
```

### cURL

```bash
curl -X POST http://localhost:8000/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "input": ["Xin ch√†o", "T√¥i y√™u l·∫≠p tr√¨nh"]
  }' | jq .
```

## üîê Authentication

### V·ªõi API Key

1. ƒê·∫∑t `API_KEY` environment variable:
   ```bash
   export API_KEY="your-secret-key"
   ```

2. G·ª≠i token trong header:
   ```bash
   Authorization: Bearer your-secret-key
   ```

### Kh√¥ng c√≥ Authentication

B·ªè tr·ªëng `API_KEY` environment variable (m·∫∑c ƒë·ªãnh)

## ‚ö° Hi·ªáu NƒÉng

### X·ª≠ l√Ω VƒÉn B·∫£n D√†i

Model c√≥ max length 256 tokens. H·ªá th·ªëng t·ª± ƒë·ªông:
1. **Chia nh·ªè** (chunk): Chia vƒÉn b·∫£n th√†nh 200 tokens/chunk v·ªõi stride 50
2. **Encode**: Encode t·ª´ng chunk
3. **Gh√©p n·ªëi** (pooling): D√πng mean pooling ƒë·ªÉ k·∫øt h·ª£p embeddings

```python
# V√≠ d·ª•: VƒÉn b·∫£n 1000 words
# ‚Üí T·ª± ƒë·ªông chia ‚Üí Encode chunks ‚Üí Mean pooling ‚Üí 1 embedding
```

### T·ªëi ∆∞u h√≥a GPU

- T·ª± ƒë·ªông s·ª≠ d·ª•ng TensorFloat32 n·∫øu s·ª≠ d·ª•ng GPU
- Batch processing cho throughput cao
- CUDA pinning memory ƒë·ªÉ tƒÉng t·ªëc

## üß™ Th·ª≠ Nghi·ªám

### Unit Tests

```bash
pytest tests/
```

### Performance Benchmark

```bash
python scripts/test_api_embedding.py
```

## üìä M√¥ h√¨nh

**Model**: `bkai-foundation-models/vietnamese-bi-encoder`

- **Ng√¥n ng·ªØ**: Ti·∫øng Vi·ªát
- **Lo·∫°i**: Bi-encoder (encoder c·∫£ 2 chi·ªÅu)
- **Output dimension**: 768
- **Max sequence length**: 256 tokens

## üêõ Troubleshooting

### 1. CUDA Out of Memory

```bash
# Gi·∫£m batch size
export BATCH_SIZE=32

# Ho·∫∑c s·ª≠ d·ª•ng CPU
export DEVICE=cpu
```

### 2. Model kh√¥ng t·∫£i ƒë∆∞·ª£c

```bash
# Ki·ªÉm tra k·∫øt n·ªëi internet
# Ho·∫∑c pre-download model:
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')"
```

### 3. Slow response

- TƒÉng `BATCH_SIZE` (n·∫øu GPU memory cho ph√©p)
- S·ª≠ d·ª•ng GPU thay v√¨ CPU
- Ki·ªÉm tra network latency

## üìù Logs

Server ghi log chi ti·∫øt:

```
using device cuda
model_name: bkai-foundation-models/vietnamese-bi-encoder
```

ƒê·ªÉ xem chi ti·∫øt h∆°n:

```bash
# V·ªõi uvicorn
uvicorn main:app --log-level debug
```

## üîÑ C·∫≠p nh·∫≠t Model

ƒê·ªÉ s·ª≠ d·ª•ng model kh√°c:

```bash
export MODEL_ID="model-name-on-huggingface"
python main.py
```

## üì¶ Docker Deployment

### Dockerfile (Production)

File `Dockerfile` ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho production v·ªõi best practices:

```dockerfile
FROM python:3.11-slim as builder

# Install system dependencies for building
RUN apt-get update && apt-get install -y \
    curl \
    build-essential \
    git \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Install uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# Set environment variables to avoid CUDA downloads
ENV CUDA_VISIBLE_DEVICES=""
ENV TORCH_USE_CUDA_DSA=0
ENV PYTORCH_NO_CUDA=1

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

WORKDIR /app

# Copy dependency files
COPY pyproject.toml uv.lock README.md ./

# Install dependencies into a virtual environment
RUN --mount=type=cache,target=/root/.cache/uv \
    uv sync --no-install-project --no-dev --link-mode=copy

# Copy the application code
COPY . .

# Install the project itself
RUN uv sync --no-dev --link-mode=copy

# Production stage
FROM python:3.11-slim

# Install only runtime system dependencies
RUN apt-get update && apt-get install -y \
    libopenblas0 \
    libgomp1 \
    curl \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Install uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# Create non-root user BEFORE copying files
RUN useradd -m -u 1000 appuser

# Copy app from builder with correct ownership
COPY --from=builder --chown=appuser:appuser /app /app

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PATH="/app/.venv/bin:$PATH"

WORKDIR /app

USER appuser

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Start application with gunicorn
CMD ["gunicorn", "main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "-b", "0.0.0.0:8000"]
```

**T√≠nh nƒÉng**:
- **Multi-stage build**: Gi·∫£m size image, t√°ch builder v√† production
- **Build cache mounting**: S·ª≠ d·ª•ng `--mount=type=cache` ƒë·ªÉ tƒÉng t·ªëc build
- **uv sync**: C√†i ƒë·∫∑t dependencies t·ª´ pyproject.toml v√† uv.lock
- **Health check**: T·ª± ƒë·ªông ki·ªÉm tra `/health` endpoint
- **Gunicorn**: Production-grade WSGI server v·ªõi 4 workers
- **Uvicorn worker**: Gunicorn s·ª≠ d·ª•ng uvicorn worker ƒë·ªÉ ch·∫°y FastAPI
- **CUDA runtime**: H·ªó tr·ª£ GPU inference v·ªõi CUDA 12.1 + cuDNN

### Dockerfile.dev (Development)

File `Dockerfile.dev` cho development v·ªõi auto-reload:

```dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    curl \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Install uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

WORKDIR /app

# Copy dependency files
COPY pyproject.toml uv.lock README.md ./

# Install dependencies
RUN --mount=type=cache,target=/root/.cache/uv \
    uv sync --link-mode=copy

# Copy the application code
COPY . .

# Set environment
ENV PATH="/app/.venv/bin:$PATH"

# Expose port
EXPOSE 8000

# Run the application with auto-reload
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

**T√≠nh nƒÉng**:
- **Auto-reload**: Ph√°t hi·ªán thay ƒë·ªïi code t·ª± ƒë·ªông
- **Single-stage**: ƒê∆°n gi·∫£n cho development
- **Build cache**: T·ªëi ∆∞u build time
- **Hot reload**: Ti·ªán khi develop

### Build Images

**Production Image**:
```bash
docker build -f Dockerfile -t bkai-embedding:latest .
docker tag bkai-embedding:latest bkai-embedding:0.1.0
```

**Development Image**:
```bash
docker build -f Dockerfile.dev -t bkai-embedding:dev .
```

### Run Container

**Production Mode**:
```bash
docker run -p 8000:8000 \
  -e DEVICE=cuda \
  -e API_KEY=your-secret-key \
  -e MODEL_ID=bkai-foundation-models/vietnamese-bi-encoder \
  --gpus all \
  --name embedding-server \
  bkai-embedding:latest
```

**Development Mode**:
```bash
docker run -it -p 8000:8000 \
  -e DEVICE=cuda \
  -e API_KEY=dev-key \
  -v $(pwd):/app \
  --gpus all \
  --name embedding-dev \
  bkai-embedding:dev
```

**CPU Only**:
```bash
docker run -p 8000:8000 \
  -e DEVICE=cpu \
  -e API_KEY=your-secret-key \
  bkai-embedding:latest
```

### Docker Compose

#### docker-compose.dev.yml

```yaml
services:
  embedding-service:
    build:
      context: .
      dockerfile: Dockerfile.dev
    ports:
      - "8000:8000"
    environment:
      - DEVICE=cuda
      - BATCH_SIZE=64
      - API_KEY=dev-key
      - MODEL_ID=bkai-foundation-models/vietnamese-bi-encoder
      - MAX_LENGTH=256
    volumes:
      - .:/app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - embedding-network

networks:
  embedding-network:
    driver: bridge
```

**Kh·ªüi ch·∫°y**:
```bash
docker-compose -f docker-compose.dev.yml up --build
```

#### docker-compose.prod.yml

```yaml
version: '3.9'

services:
  embedding-service:
    build:
      context: .
      dockerfile: Dockerfile
    restart: always
    ports:
      - "8000:8000"
    environment:
      - DEVICE=cuda
      - BATCH_SIZE=64
      - API_KEY=${API_KEY}
      - MODEL_ID=bkai-foundation-models/vietnamese-bi-encoder
      - MAX_LENGTH=256
      - CUDA_LAUNCH_BLOCKING=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 16G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - embedding-network
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "10"

networks:
  embedding-network:
    driver: bridge
```

**Kh·ªüi ch·∫°y**:
```bash
export API_KEY="your-production-key"
docker-compose -f docker-compose.prod.yml up -d
```

### Docker Commands

**View logs**:
```bash
docker logs embedding-server
docker logs -f embedding-server  # Follow logs
```

**Stop container**:
```bash
docker stop embedding-server
docker rm embedding-server
```

**Check status**:
```bash
docker ps
docker inspect embedding-server
```

**Execute command in container**:
```bash
docker exec embedding-server python -c "import torch; print(torch.cuda.is_available())"
```

### Docker Network

N·∫øu c·∫ßn k·∫øt n·ªëi v·ªõi c√°c service kh√°c:

```yaml
services:
  embedding-service:
    networks:
      - backend-network
  
  backend:
    networks:
      - backend-network

networks:
  backend-network:
    driver: bridge
```

T·ª´ backend service, c√≥ th·ªÉ g·ªçi:
```python
response = requests.post("http://embedding-service:8000/v1/embeddings", ...)
```

### Docker Registry (Push to Docker Hub)

```bash
# Login
docker login

# Tag image
docker tag bkai-embedding:latest username/bkai-embedding:latest
docker tag bkai-embedding:latest username/bkai-embedding:0.1.0

# Push
docker push username/bkai-embedding:latest
docker push username/bkai-embedding:0.1.0

# Pull
docker pull username/bkai-embedding:latest
```

## ü§ù ƒê√≥ng G√≥p

ƒê·ªÉ c·∫£i ti·∫øn:

1. Fork repository
2. T·∫°o feature branch
3. Commit changes
4. Push to branch
5. Open Pull Request

## üìÑ License

MIT License

## üìß Support

C√≥ v·∫•n ƒë·ªÅ? Li√™n h·ªá ho·∫∑c m·ªü issue tr√™n repository.

## üóÇÔ∏è C·∫•u Tr√∫c M√£

```python
# main.py

# 1. Config Class
class Config:
    """Qu·∫£n l√Ω c·∫•u h√¨nh t·ª´ environment variables"""
    
# 2. EmbeddingModel Class
class EmbeddingModel:
    """Qu·∫£n l√Ω model encoding"""
    
# 3. EmbeddingAPI Class
class EmbeddingAPI:
    """FastAPI application"""
    
# 4. EmbeddingRequest Schema
class EmbeddingRequest(BaseModel):
    """Request schema cho API"""
    
# 5. Main Entry Point
if __name__ == "__main__":
    config = Config()
    model = EmbeddingModel(config)
    api = EmbeddingAPI(config, model)
    uvicorn.run(api.app, host="0.0.0.0", port=8000)
```

## ‚ú® T√≠nh nƒÉng T∆∞∆°ng Lai

- [ ] Caching embeddings
- [ ] Batch processing optimization
- [ ] Metrics/Monitoring
- [ ] Multi-model support
- [ ] Async processing queue
- [ ] Database integration

---

**Version**: 0.1.0  
**Last Updated**: October 2025

